{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ba63bf0a-87cc-455d-b39c-333eda0d8740",
   "metadata": {},
   "source": [
    "## Course Assignment Instructions\n",
    "You should have Python (version 3.8 or later) and Jupyter Notebook installed to complete this assignment. You will write code in the empty cell/cells below the problem. While most of this will be a programming assignment, some questions will ask you to \"write a few sentences\" in markdown cells. \n",
    "\n",
    "Submission Instructions:\n",
    "\n",
    "Create a labs directory in your personal class repository (e.g., located in your home directory)\n",
    "Clone the class repository\n",
    "Copy this Jupyter notebook file (.ipynb) into your repo/labs directory\n",
    "Make your edits, commit changes, and push to your repository\n",
    "All submissions must be pushed before the due date to avoid late penalties. \n",
    "\n",
    "Labs are graded out of a 100 pts. Each day late is -10. For a max penalty of -50 after 5 days. From there you may submit the lab anytime before the semester ends for a max score of 50.  \n",
    "\n",
    "Lab 3 is due on 2/28/25"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f2b725e-2333-4617-9c7f-d7898279f72f",
   "metadata": {},
   "source": [
    "## Perceptron\n",
    "\n",
    "You will code the \"perceptron learning algorithm\" for arbitrary number of features p. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd67b310-fe96-4899-b1dc-86fe2fd9e5d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def perceptron_learning_algorithm(Xinput, y_binary, MAX_ITER=1000, learning_rate=1, w=None):\n",
    "    # Convert input to a NumPy array and add bias column\n",
    "    Xinput = np.asarray(Xinput)\n",
    "    n = Xinput.shape[0]\n",
    "    # Add a column of ones to the left for the bias term\n",
    "    X = np.column_stack((np.ones(n), Xinput))\n",
    "    p = X.shape[1]\n",
    "    \n",
    "    # Initialize the weight vector \n",
    "    if w is None:\n",
    "        w = np.zeros(p)\n",
    "    else:\n",
    "        w = np.asarray(w)\n",
    "        if w.shape[0] != p:\n",
    "            raise ValueError(f\"Initial weight vector 'w' must be of length {p} (including bias term).\")\n",
    "    \n",
    "    # Iterate up to MAX_ITER times\n",
    "    for iter in range(1, MAX_ITER + 1):\n",
    "        error_flag = False  # Flag to check if any misclassification occurred in this iteration\n",
    "        \n",
    "        # Loop over each training example\n",
    "        for i in range(n):\n",
    "            x_i = X[i, :]\n",
    "            # Compute the activation\n",
    "            activation = np.sum(x_i * w)\n",
    "            # Determine the predicted class (using threshold 0)\n",
    "            yhat_i = 1 if activation > 0 else 0\n",
    "            \n",
    "            # If the prediction is wrong, update the weight vector\n",
    "            if yhat_i != y_binary[i]:\n",
    "                w = w + learning_rate * (y_binary[i] - yhat_i) * x_i\n",
    "                error_flag = True\n",
    "        \n",
    "        # Stop early if no misclassifications were found in this iteration\n",
    "        if not error_flag:\n",
    "            print(f\"Convergence achieved at iteration {iter}\")\n",
    "            break\n",
    "            \n",
    "    return w\n",
    "\n",
    "# Specs\n",
    "# Xinput should be a 2D array-like structure (or a 1D array if you have only one feature per observation)\n",
    "# y_binary should be a binary vector (list or numpy array) of the same length as the number of rows in Xinput.\n",
    "# For example:\n",
    "# Xinput = np.array([[2.5], [3.0], [3.5]])\n",
    "# y_binary = np.array([0, 1, 1])\n",
    "# weights = perceptron_learning_algorithm(Xinput, y_binary)\n",
    "# print(\"Learned weights:\", weights)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02aacefb-7b79-4059-8d56-780be918ca85",
   "metadata": {},
   "source": [
    "To understand what the algorithm is doing - linear \"discrimination\" between two response categories, we can draw a picture. First let's make up some very simple training data D."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72e8a89e-c8a5-487a-9d4f-0cbf78b0ae7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "Xy_simple = pd.DataFrame({\n",
    "    'response': [0, 0, 0, 1, 1, 1],\n",
    "    'first_feature': [1, 1, 2, 3, 3, 4],\n",
    "    'second_feature': [1, 2, 1, 3, 4, 3]\n",
    "})\n",
    "\n",
    "print(Xy_simple)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a07a11bd-2adc-4a2b-b238-a187b030113f",
   "metadata": {},
   "source": [
    "We haven't spoken about visualization yet, but it is important we do some of it now. Thus, I will write this code for you and you will just run it. First we load the visualization library we're going to use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f265d51-181c-47fa-a6bb-c524ff92fc11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from plotnine import ggplot, aes, geom_point\n",
    "\n",
    "# Create the dataset equivalent to R's Xy_simple\n",
    "Xy_simple = pd.DataFrame({\n",
    "    'response': [0, 0, 0, 1, 1, 1],\n",
    "    'first_feature': [1, 1, 2, 3, 3, 4],\n",
    "    'second_feature': [1, 2, 1, 3, 4, 3]\n",
    "})\n",
    "\n",
    "# Create the visualization\n",
    "simple_viz_obj = (ggplot(Xy_simple, aes(x='first_feature', y='second_feature', color='factor(response)'))\n",
    "                  + geom_point(size=5))\n",
    "\n",
    "# Display the plot (in a Jupyter Notebook, this will render inline)\n",
    "print(simple_viz_obj)\n",
    "simple_viz_obj"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae3db17d-8e0a-43a2-bf7a-5e13a5fc24e4",
   "metadata": {},
   "source": [
    "Explain this picture ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11885a6a-d460-46a1-b47d-3563837c9495",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f9f0255a-6827-481a-a435-d2dfd6fc74d4",
   "metadata": {},
   "source": [
    "Now, let us run the algorithm and see what happens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "878d262f-c66d-48b4-9f70-cd887602e3d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the input matrix by combining the two feature columns\n",
    "Xinput = np.column_stack((Xy_simple['first_feature'], Xy_simple['second_feature']))\n",
    "\n",
    "# Create the binary response vector, where response==1 becomes 1 and otherwise 0\n",
    "y_binary = (Xy_simple['response'] == 1).astype(int).values\n",
    "\n",
    "# Run the perceptron learning algorithm (assuming it is already defined)\n",
    "w_vec_simple_per = perceptron_learning_algorithm(Xinput, y_binary)\n",
    "\n",
    "print(w_vec_simple_per)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fc841a2-fa2b-4991-84aa-b03768834acd",
   "metadata": {},
   "source": [
    "Explain this output. What do the numbers mean? What is the intercept of this line and the slope? You will have to do some algebra."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42979026-4629-4a61-87e1-bc13324499d9",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "961f0768-9301-4793-ad02-cd700b01f85d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from plotnine import geom_abline\n",
    "\n",
    "# Assuming w_vec_simple_per is a NumPy array or list with three elements:\n",
    "# Python indexing: w_vec_simple_per[0] is the bias, w_vec_simple_per[1] is for first_feature,\n",
    "# and w_vec_simple_per[2] is for second_feature.\n",
    "# The R code computes:\n",
    "#   intercept = -w_vec_simple_per[1] / w_vec_simple_per[3]\n",
    "#   slope     = -w_vec_simple_per[2] / w_vec_simple_per[3]\n",
    "# In Python, that becomes:\n",
    "#   intercept = -w_vec_simple_per[0] / w_vec_simple_per[2]\n",
    "#   slope     = -w_vec_simple_per[1] / w_vec_simple_per[2]\n",
    "\n",
    "simple_perceptron_line = geom_abline(\n",
    "    intercept = -w_vec_simple_per[0] / w_vec_simple_per[2],\n",
    "    slope = -w_vec_simple_per[1] / w_vec_simple_per[2],\n",
    "    color = \"orange\"\n",
    ")\n",
    "\n",
    "# Assuming simple_viz_obj is your base Plotnine plot object:\n",
    "plot = simple_viz_obj + simple_perceptron_line\n",
    "print(plot)\n",
    "plot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d1868ef-f166-44f1-bad6-df7207c237cc",
   "metadata": {},
   "source": [
    "Explain this picture. Why is this line of separation not \"satisfying\" to you?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a303c667-b274-4a7f-95f4-7dc62f9043f7",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c09ffa57-fa31-41d2-b1d2-8a6aecd41427",
   "metadata": {},
   "source": [
    "Extra Credit (+5): Program the maximum-margin hyperplane perceptron that provides the best linear discrimination model for linearly separable data. Make sure you provide clear comments explain your code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4a3c939-83e1-4d4d-bbd3-a7c0a1112814",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e8374b8f-ca34-434a-b34a-83e785cc541f",
   "metadata": {},
   "source": [
    "## Support Vector Machine vs. Perceptron\n",
    "\n",
    "We recreate the data from the previous example and visualize it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3cbc975-7e9c-4da7-a5ed-ac24b81a57cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the dataset equivalent to R's Xy_simple\n",
    "Xy_simple = pd.DataFrame({\n",
    "    'response': [0, 0, 0, 1, 1, 1],\n",
    "    'first_feature': [1, 1, 2, 3, 3, 4],\n",
    "    'second_feature': [1, 2, 1, 3, 4, 3]\n",
    "})\n",
    "\n",
    "# Create the visualization\n",
    "simple_viz_obj = (ggplot(Xy_simple, aes(x='first_feature', y='second_feature', color='factor(response)'))\n",
    "                  + geom_point(size=5))\n",
    "\n",
    "# Display the plot (in a Jupyter Notebook, this will render inline)\n",
    "print(simple_viz_obj)\n",
    "simple_viz_obj"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0390f3d-4500-41d8-8852-b029b5c45277",
   "metadata": {},
   "source": [
    "Use the import SVC from the sklearn package to fit an SVM model to the simple data. the SVC function to create the model, pass in the data frame, set kernel to be `linear` for the linear SVM and don't scale the covariates. Call the model object `svm_model`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34bf3d1a-8fd2-4cc0-8131-1c77782c0c74",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7a5617a-8c6e-46a4-8b85-dd19bc6b3811",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Define predictors (all columns except 'response') and response\n",
    "X = Xy_simple[['first_feature', 'second_feature']]\n",
    "y = Xy_simple['response']\n",
    "\n",
    "# Fit a linear SVM model (with kernel 'linear' and without scaling the covariates)\n",
    "svm_model = SVC(kernel='linear')\n",
    "svm_model.fit(X, y)\n",
    "\n",
    "# The fitted model is stored in svm_model\n",
    "print(\"SVM model coefficients and intercept:\")\n",
    "# For a linear SVM, the coefficients are stored in svm_model.coef_ and the intercept in svm_model.intercept_\n",
    "print(\"Coefficients:\", svm_model.coef_)\n",
    "print(\"Intercept:\", svm_model.intercept_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b861ec15-9fa9-4f2c-9926-71066453f710",
   "metadata": {},
   "source": [
    "and then use the following code to visualize the line in purple:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d15cf7ab-6136-4929-b20e-e0fb5c8525f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from plotnine import ggplot, aes, geom_point, geom_abline, theme_minimal\n",
    "\n",
    "\n",
    "# Prepare the predictors and response\n",
    "X = Xy_simple[['first_feature', 'second_feature']]\n",
    "y = Xy_simple['response']\n",
    "\n",
    "# Fit the linear SVM model (kernel='linear', no scaling done)\n",
    "svm_model = SVC(kernel='linear')\n",
    "svm_model.fit(X, y)\n",
    "\n",
    "# In scikit-learn, the decision function is: f(x) = w1*x1 + w2*x2 + b,\n",
    "# where svm_model.coef_ = [w1, w2] and svm_model.intercept_ = [b].\n",
    "# To mimic the R code:\n",
    "# w_vec_simple_svm = c(svm_model$rho, -t(svm_model$coefs) %*% support_vectors)\n",
    "# Note: In e1071, svm_model$rho is negative of the intercept.\n",
    "# For our purposes, we construct a weight vector as: [b, w1, w2]\n",
    "w_vec_simple_svm = np.concatenate((svm_model.intercept_, svm_model.coef_.flatten()))\n",
    "\n",
    "# For plotting the decision boundary, we solve:\n",
    "# w1 * x + w2 * y + b = 0  ->  y = -b/w2 - (w1/w2)*x.\n",
    "# So, the intercept for the line is -b/w2 and the slope is -w1/w2.\n",
    "line_intercept = -w_vec_simple_svm[0] / w_vec_simple_svm[2]\n",
    "line_slope = -w_vec_simple_svm[1] / w_vec_simple_svm[2]\n",
    "\n",
    "# Create the decision boundary line in Plotnine:\n",
    "simple_svm_line = geom_abline(intercept=line_intercept, slope=line_slope, color=\"purple\")\n",
    "\n",
    "# Create the base plot for the data (using factor conversion for color)\n",
    "simple_viz_obj = (ggplot(Xy_simple, aes(x=\"first_feature\", y=\"second_feature\", color=\"factor(response)\"))\n",
    "                  + geom_point(size=5)\n",
    "                  + simple_svm_line\n",
    "                  + theme_minimal())\n",
    "\n",
    "print(simple_viz_obj)\n",
    "simple_viz_obj"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "615338e7-49f0-4e41-b90b-b4e0012ed2ca",
   "metadata": {},
   "source": [
    "Source the `perceptron_learning_algorithm` then run the following to fit the perceptron and plot its line in orange with the SVM's line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "154629e4-1ac2-48a1-b648-0c6295bc946f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the decision boundary for the perceptron.\n",
    "# In R, w_vec_simple_per[1] is bias, [2] for first_feature, [3] for second_feature.\n",
    "# Python is 0-indexed, so:\n",
    "#   intercept = -w_vec_simple_per[0] / w_vec_simple_per[2]\n",
    "#   slope     = -w_vec_simple_per[1] / w_vec_simple_per[2]\n",
    "perceptron_intercept = -w_vec_simple_per[0] / w_vec_simple_per[2]\n",
    "perceptron_slope = -w_vec_simple_per[1] / w_vec_simple_per[2]\n",
    "\n",
    "\n",
    "# Assume that simple_svm_line and simple_viz_obj were defined in previous code.\n",
    "# For example, simple_viz_obj might be:\n",
    "simple_viz_obj = (ggplot(Xy_simple, aes(x=\"first_feature\", y=\"second_feature\", color=\"factor(response)\"))\n",
    "                  + geom_point(size=5)\n",
    "                  + theme_minimal())\n",
    "\n",
    "# Simple_svm_line (the SVM decision boundary) ... this was also defined in the previous cell.\n",
    "simple_svm_line = geom_abline(intercept=line_intercept, slope=line_slope, color=\"purple\")\n",
    "\n",
    "# Combine the plots: data, SVM line (purple) and perceptron line (orange)\n",
    "final_plot = simple_viz_obj + simple_perceptron_line + simple_svm_line\n",
    "\n",
    "print(final_plot)\n",
    "final_plot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "402712aa-df0f-4f8c-b528-6e3e182d4c01",
   "metadata": {},
   "source": [
    "Is this SVM line a better fit than the perceptron?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e7bba9b-30de-469b-9e81-b6a361414195",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f430fd6a-d4fd-4838-bac7-56d338dcb4ce",
   "metadata": {},
   "source": [
    "Extra Credit (+5): Now write your own implementation of the linear support vector machine algorithm using the Vapnik objective function discussed in 342."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2170ab5e-5752-4dde-a25a-778d5b65d973",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This function should implement the hinge-loss + maximum margin linear support vector machine algorithm of Vladimir Vapnik (1963).\n",
    "#parameters to include: Xinput, y_binary, MAX_ITER, lambda(give this another name since lambda is a keyword in python). \n",
    "def linear_svm_learning_algorithm():"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d468d01e-12e5-464b-8e7d-3fce92bc8ae2",
   "metadata": {},
   "source": [
    "## Multinomial Classification using KNN\n",
    "\n",
    "Write a k = 1 nearest neighbor algorithm using the Euclidean distance function. The following comments are standard \"Roxygen\" format for documentation. Hopefully, we will get to packages at some point and we will go over this again. It is your job also to fill in this documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10d9bc62-203d-4023-baae-b0694fd3ad4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def nn_algorithm_predict(Xinput, y_binary, Xtest):\n",
    "    \"\"\"\n",
    "    1-Nearest Neighbor Predictor using Euclidean Distance.\n",
    "    \n",
    "    This function implements a simple 1-nearest neighbor algorithm. Given a matrix of training \n",
    "    features (Xinput), a vector of binary class labels (y_binary), and a test observation (Xtest),\n",
    "    it computes the Euclidean distance between Xtest and each training observation, and returns the \n",
    "    class label corresponding to the nearest neighbor.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    Xinput : array-like\n",
    "        A 2D array (or something convertible to a NumPy array) where each row represents an observation.\n",
    "    y_binary : array-like\n",
    "        A vector of binary class labels corresponding to each row in Xinput.\n",
    "    Xtest : array-like\n",
    "        A 1D array representing the features of the test observation.\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    int or float:\n",
    "        The predicted class label for the test observation.\n",
    "    \"\"\"\n",
    "    \n",
    "# Example usage:\n",
    "# Xinput = np.array([[1, 2], [2, 3], [3, 4]])\n",
    "# y_binary = np.array([0, 1, 0])\n",
    "# Xtest = np.array([2.5, 3.5])\n",
    "# print(nn_algorithm_predict(Xinput, y_binary, Xtest))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9f4c0f5-e00d-45d7-afc6-bcd66e5544eb",
   "metadata": {},
   "source": [
    "Write a few tests to ensure it actually works importing  'assert_equals' from numpy.testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "797fd8d5-0660-4b2a-baf8-041b347b132a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.testing import assert_equal\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fae311f-d4b9-40fc-ab08-f1039e0b25ec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d7c2a36c-f0b0-4f78-bc71-f2c78a2b8c00",
   "metadata": {},
   "source": [
    "We now add an argument `d` representing any legal distance function to the `nn_algorithm_predict` function. Update the implementation so it performs NN using that distance function. Set the default function to be the Euclidean distance in the original function. Also, alter the documentation in the appropriate places."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fbdb813-bc8a-4e9a-81a3-1d252016977a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def nn_algorithm_predict(Xinput, y_binary, Xtest, d=lambda x, y: np.sqrt(np.sum((x - y)**2))):\n",
    "    \"\"\"\n",
    "    1-Nearest Neighbor Predictor with Custom Distance Function.\n",
    "\n",
    "    This function implements a simple 1-nearest neighbor algorithm using a customizable distance \n",
    "    function. Given a matrix of training features (Xinput), a vector of class labels (y_binary), \n",
    "    and a test observation (Xtest), it computes the distance from Xtest to each training observation \n",
    "    using the provided distance function (defaulting to Euclidean distance), and returns the label \n",
    "    corresponding to the nearest neighbor.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    Xinput : array-like\n",
    "        A 2D array where each row represents an observation.\n",
    "    y_binary : array-like\n",
    "        A vector of class labels corresponding to each row in Xinput.\n",
    "    Xtest : array-like\n",
    "        A 1D array representing the features of the test observation.\n",
    "    d : function, optional\n",
    "        A function that takes two numeric arrays as input and returns a numeric distance between them.\n",
    "        The default is a function that computes the Euclidean distance.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    The predicted class label for the test observation.\n",
    "    \"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db4f027d-9061-420e-b81a-3c88723045f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 1: Exact Match Test\n",
    "# If the test observation exactly matches a training observation, the corresponding label should be returned.\n",
    "Xinput_test1 = np.array([[1, 1],\n",
    "                         [2, 2],\n",
    "                         [3, 3]])\n",
    "y_binary_test1 = np.array([0, 1, 0])\n",
    "Xtest1 = np.array([1, 1])\n",
    "pred1 = nn_algorithm_predict(Xinput_test1, y_binary_test1, Xtest1)\n",
    "assert_equal(pred1, 0)\n",
    "print(\"Test 1 passed: Exact match returns the correct label.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df70b21c-a50e-488f-847b-f1367730c387",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Chebyshev distance function\n",
    "chebyshev_distance = lambda x, y: np.max(np.abs(x - y))\n",
    "\n",
    "# Create training data where the nearest neighbor is uniquely determined using Chebyshev distance:\n",
    "Xinput_test_alt = np.array([[1, 2], [3, 4], [5, 6]])\n",
    "y_binary_test_alt = np.array([0, 1, 0])\n",
    "# Choose a test observation that is clearly closest to the second training observation:\n",
    "Xtest_alt = np.array([3.1, 4.1])\n",
    "# Compute distances:\n",
    "# For [1,2]: max(|3.1-1|, |4.1-2|) = max(2.1, 2.1) = 2.1\n",
    "# For [3,4]: max(|3.1-3|, |4.1-4|) = max(0.1, 0.1) = 0.1\n",
    "# For [5,6]: max(|3.1-5|, |4.1-6|) = max(1.9, 1.9) = 1.9\n",
    "# The nearest neighbor is the second observation (with label 1).\n",
    "pred_alt = nn_algorithm_predict(Xinput_test_alt, y_binary_test_alt, Xtest_alt, d=chebyshev_distance)\n",
    "assert_equal(pred_alt, 1)\n",
    "print(\"Alternate Test 2 passed: Chebyshev distance with unique nearest neighbor returns the correct label.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "424e2a3d-b186-41fc-8102-6e0f2c2ecb53",
   "metadata": {},
   "source": [
    "## Regression via OLS with one feature\n",
    "\n",
    "Let's quickly recreate the sample data set from practice lecture 7:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64fe13a4-de9c-494e-8727-5616c11047c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.random.seed(1984)\n",
    "n = 20\n",
    "x = np.random.uniform(size=n)\n",
    "beta_0 = 3\n",
    "beta_1 = -2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6dfc2f0-329a-4e47-8beb-3b1c87315ebe",
   "metadata": {},
   "source": [
    "Compute $h^*$ as `h_star_x`, then draw $\\epsilon$ from an iid $N(0, 0.33^2)$ distribution as `epsilon`, then compute the vector $y$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f118bae-5d35-41e4-af8e-e2160b728f12",
   "metadata": {},
   "outputs": [],
   "source": [
    "h_star_x = \n",
    "epsilon = np.random.normal()\n",
    "y = "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd34aa5f-b64c-4636-95e4-aabff7e588a6",
   "metadata": {},
   "source": [
    "Graph the data by running the following chunk:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd02e565-4f8b-4a40-951c-2d39e447ecee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from plotnine import ggplot, aes, geom_point\n",
    "\n",
    "# Assuming x and y are defined (e.g., as NumPy arrays or lists)\n",
    "simple_df = pd.DataFrame({'x': x, 'y': y})\n",
    "simple_viz_obj = (ggplot(simple_df, aes(x='x', y='y'))\n",
    "                  + geom_point(size=2))\n",
    "simple_viz_obj"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d2129c3-b91b-42c6-9b38-63216ae07743",
   "metadata": {},
   "source": [
    "Does this make sense given the values of beta_0 and beta_1?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b440f5f-5596-4d16-9d6c-6753d057d52f",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1a8c17b4-214b-4d18-b6e2-3221afaa66e9",
   "metadata": {},
   "source": [
    "Write a function `my_simple_ols` that takes in a vector `x` and vector `y` and returns a list that contains the `b_0` (intercept), `b_1` (slope), `yhat` (the predictions), `e` (the residuals), `SSE`, `SST`, `MSE`, `RMSE` and `Rsq` (for the R-squared metric). Internally, you can only use the functions `sum` and `length` and other basic arithmetic operations. You should throw errors if the inputs are non-numeric or not the same length. You should also name the class of the return value `my_simple_ols_obj` by using the `class` function as a setter. No need to create ROxygen documentation here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "939bcb45-877d-40c5-b234-469621cc57e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "\n",
    "def my_simple_ols_mod(x, y):\n",
    "   \n",
    "    # Build the model dictionary\n",
    "    model = {\n",
    "        \"b_0\": b_0,\n",
    "        \"b_1\": b_1,\n",
    "        \"yhat\": yhat,\n",
    "        \"e\": e,\n",
    "        \"SSE\": SSE,\n",
    "        \"SST\": SST,\n",
    "        \"MSE\": MSE,\n",
    "        \"RMSE\": RMSE,\n",
    "        \"Rsq\": Rsq,\n",
    "        \"class\": \"my_simple_ols_obj\"\n",
    "    }\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Assign the output of the function to a variable\n",
    "my_simple_ols_obj = my_simple_ols_mod(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c7a9f73-715e-4cd3-83df-d478804faa68",
   "metadata": {},
   "source": [
    "Verify your computations are correct for the vectors `x` and `y` from the first chunk using the OLS function from statsmodels. To do this you will import statsmodels.api ... this is the primary interface to statsmodels library in Python and it provides a collection of classes and functions for estimating various statistical models (i.e. OLS, logistic regression, time series etc.), performing statistical tests, and conducting data exploration without having to import individual submodules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7ed0c66-9688-48b5-bdd1-3ba3c2541f6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "from numpy.testing import assert_allclose\n",
    "\n",
    "# Fit the linear model using statsmodels (equivalent to R's lm)\n",
    "X = sm.add_constant()   # add intercept column to x\n",
    "lm_mod = sm.OLS().fit()\n",
    "\n",
    "# Extract parameters and metrics from statsmodels output\n",
    "lm_b0 = lm_mod.params[0]\n",
    "lm_b1 = lm_mod.params[1]\n",
    "lm_RMSE = np.sqrt(lm_mod.mse_resid)\n",
    "lm_Rsq = lm_mod.rsquared\n",
    "\n",
    "# Run tests using assert_allclose from numpy.testing\n",
    "assert_allclose(my_simple_ols_obj[\"b_0\"], lm_b0, rtol=1e-4)\n",
    "assert_allclose(my_simple_ols_obj[\"b_1\"], lm_b1, rtol=1e-4)\n",
    "assert_allclose(my_simple_ols_obj[\"RMSE\"], lm_RMSE, rtol=1e-4)\n",
    "assert_allclose(my_simple_ols_obj[\"Rsq\"], lm_Rsq, rtol=1e-4)\n",
    "\n",
    "print(\"All tests passed!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cd20a82-b28e-4e40-91a3-eeae44d19da8",
   "metadata": {},
   "source": [
    "Verify that the average of the residuals is 0 using the `assert_allclose()`. Hint: use the syntax above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0f1c6b3-4fd4-4d98-9ed8-401a97b43711",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.mean())\n",
    "\n",
    "print(\"Residual mean test passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "763b0e66-38e7-4e79-b077-f11b4e41ce42",
   "metadata": {},
   "source": [
    "Create the $X$ matrix for this data example. Make sure it has the correct dimension. Use np.array, np.column_stack, and np.ones for the intercept. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bd2905f-b40e-4997-9e41-48db6d84625c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "aef0b137-05d2-492b-b4bd-221186bf8430",
   "metadata": {},
   "source": [
    "Use the `dmatrix` from the patsy module to compute the matrix `X` and verify it is the same as your manual construction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e67ad82-68bb-410e-8c15-f4176bb3add2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from patsy import dmatrix\n",
    "\n",
    "# Construction using patsy's dmatrix (automatically adds an intercept)\n",
    "X_model = dmatrix()\n",
    "\n",
    "# Display the model matrix from patsy\n",
    "\n",
    "# Verify that the two matrices are equivalent\n",
    "# Convert X_model to a numpy array before comparison\n",
    "print(\"Are X_manual and X_model equivalent?\")\n",
    "print(np.allclose(X_manual, np.asarray(X_model)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "202079f3-10c1-4874-a7a9-1ace3a10025c",
   "metadata": {},
   "source": [
    "Create a prediction method `g` that takes in a vector `x_star` and `my_simple_ols_obj`, an object of type `my_simple_ols_obj` and predicts y values for each entry in `x_star`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e50189dd-f9da-4350-bb80-816b7745a105",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def g(my_simple_ols_obj, x_star):\n",
    "    # Ensure x_star is a NumPy array for vectorized computation\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10fd97aa-6af3-4720-97c9-719222bc2e5e",
   "metadata": {},
   "source": [
    "Use this function to verify that when predicting for the average x, you get the average y."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e94244a8-0b90-47df-8a64-cee7522a4964",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify that predicting for the average x gives the average y\n",
    "assert_allclose(g(my_simple_ols_obj, np.mean(x)), np.mean(y), rtol=1e-4)\n",
    "print(\"Test passed: Prediction for mean(x) equals mean(y).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de826a9d-59e8-49cf-a4a7-687ecea9d684",
   "metadata": {},
   "source": [
    "In class we spoke about error due to ignorance, misspecification error and estimation error. Show that as n grows, estimation error shrinks. Let us define an error metric that is the difference between b_0 and b_1 and beta_0 and beta_1. How about ||b - beta||^2 where the quantities are now the vectors of size two. Show as n increases, this shrinks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40a00553-a46d-41dc-bfb2-60230c6f5853",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "beta_0 = 3\n",
    "beta_1 = -2\n",
    "beta = np.array([beta_0, beta_1])\n",
    "ns = \n",
    "errors = np.empty(len(ns))\n",
    "\n",
    "for i, n in enumerate(ns):\n",
    "    # Generate x uniformly on [0,1)\n",
    "    x = np.random.uniform(size=n)\n",
    "    h_star_x = beta_0 + beta_1 * x\n",
    "    epsilon = np.random.normal(loc=0, scale=0.33, size=n)\n",
    "    y = h_star_x + epsilon\n",
    "    \n",
    "    # Fit our simple OLS model (assumed to be defined)\n",
    "    \n",
    "    \n",
    "    # Compute squared Euclidean error between beta and b\n",
    "    errors[i] = np.sum((beta - b)**2)\n",
    "\n",
    "print(\"Errors:\", errors)\n",
    "print(\"Log10(Errors):\", np.log10(errors))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cef75789-c55e-4c04-b17b-ce5b73e654f1",
   "metadata": {},
   "source": [
    "We are now going to repeat one of the first linear model building exercises in history --- that of Sir Francis Galton in 1886. First uncomment the line below and install the pydataset package. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4d04f62-9fae-49e9-b4e4-ad9039464bd3",
   "metadata": {},
   "source": [
    "Now import the galton dataset by running the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55ce9e75-c7b9-4b8b-84c0-236075db9d96",
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "galton = sm.datasets.get_rdataset(\"Galton\", \"HistData\").data\n",
    "\n",
    "print(galton.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89aff1c3-1cd0-47f7-8bad-a52843a9ac84",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import ydata_profiling\n",
    "\n",
    "# Load the Galton dataset from statsmodels\n",
    "galton = sm.datasets.get_rdataset(\"Galton\", \"HistData\").data\n",
    "galton_df = pd.DataFrame(galton)\n",
    "\n",
    "# Summary statistics using `describe()`\n",
    "summary_stats = galton_df.describe()\n",
    "\n",
    "# Display summary statistics\n",
    "print(summary_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "521a61db-78dc-4233-9c2a-f28914e3036c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate an advanced report using `ydata_profiling`\n",
    "profile = ydata_profiling.ProfileReport(galton_df, title=\"Galton_Summary\", explorative=True)\n",
    "\n",
    "# Generate the profiling report (Uncomment to generate HTML file)\n",
    "profile.to_file(\"Galton_Summary.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26591063-fa97-4ed6-9f78-084bf011f01e",
   "metadata": {},
   "source": [
    "Find the average height (include both parents and children in this computation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7b275b6-d328-47c1-abb2-b04b1b818da2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming 'galton' is a pandas DataFrame with columns 'parent' and 'child'\n",
    "avg_height = pd.concat()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33ab18e2-84b5-439b-a468-bc7d0b356c84",
   "metadata": {},
   "source": [
    "If you were predicting child height from parent and you were using the null model, what would the RMSE be of this model be?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9506f917-752d-4dcd-a9f1-99fd0a753ebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming 'galton' is a pandas DataFrame with a column 'child'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25f6120d-6e61-4524-baf9-542051c928a6",
   "metadata": {},
   "source": [
    "Note that in Math 241 you learned that the sample average is an estimate of the \"mean\", the population expected value of height. We will call the average the \"mean\" going forward since it is probably correct to the nearest tenth of an inch with this amount of data.\n",
    "\n",
    "Run a linear model attempting to explain the childrens' height using the parents' height. Use `lm` and use the R formula notation. Compute and report b_0, b_1, RMSE and R^2. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "513bf3bf-c319-4e3d-b91a-f487991fda62",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1e17ef20-2c0b-4ee0-99ea-2d0a907cc8be",
   "metadata": {},
   "source": [
    "1) Interpret all four quantities: b_0, b_1, RMSE and R^2. Use the correct units of these metrics in your answer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4582a8b3-fe5e-4872-b324-82f79bfe5f4c",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "71cc9d4b-41d4-47e2-9e5d-e17f212f9fb1",
   "metadata": {},
   "source": [
    "2) How good is this model? How well does it predict? Discuss."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "899186a4-0671-4e9e-a4cf-dcfc50ab126f",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "304798ad-a0e8-4b02-8cf8-73af555537bc",
   "metadata": {},
   "source": [
    "3) It is reasonable to assume that parents and their children have the same height? Explain why this is reasonable using basic biology and common sense."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7defe10e-25c3-4e15-a0cd-cc36ca20d5e8",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8f35b06f-0e1c-40f7-86db-e16449b2b64b",
   "metadata": {},
   "source": [
    "Let's plot (a) the data in D as black dots, (b) your least squares line defined by b_0 and b_1 in blue, (c) the theoretical line beta_0 and beta_1 if the parent-child height equality held in red and (d) the mean height in green."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7a38244-cfee-4bdc-9c2b-f3a1fd344939",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as pl\n",
    "%matplotlib inline\n",
    "\n",
    "# Define the x-range for our lines (set to match xlim in R)\n",
    "x_range = np.array([63.5, 72.5])\n",
    "\n",
    "# Create a new figure\n",
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "# (a) Plot the data as black dots. To mimic geom_jitter, you can add a small random noise.\n",
    "jitter_strength = 0.2  # adjust as needed\n",
    "plt.scatter(galton['parent'] + np.random.uniform(-jitter_strength, jitter_strength, size=len(galton)),\n",
    "            galton['child'] + np.random.uniform(-jitter_strength, jitter_strength, size=len(galton)),\n",
    "            color='black', label='Data')\n",
    "\n",
    "# (b) Plot the least squares line (blue) using your computed b_0 and b_1.\n",
    "y_ls = b_0 + b_1 * x_range\n",
    "plt.plot(x_range, y_ls, color='blue', linewidth=2, label='LS Line')\n",
    "\n",
    "# (c) Plot the theoretical line: child = parent (red).\n",
    "plt.plot(x_range, x_range, color='red', linewidth=2, label='Theoretical Line')\n",
    "\n",
    "# (d) Plot the horizontal line at the average height (green).\n",
    "plt.axhline(y=avg_height, color='darkgreen', linewidth=2, label='Mean Height')\n",
    "\n",
    "# Set the x and y limits\n",
    "plt.xlim(63.5, 72.5)\n",
    "plt.ylim(63.5, 72.5)\n",
    "\n",
    "# Set the aspect ratio to equal\n",
    "plt.gca().set_aspect('equal', adjustable='box')\n",
    "\n",
    "plt.xlabel('Parent Height')\n",
    "plt.ylabel('Child Height')\n",
    "plt.title('Galton Data: Parent vs. Child Height')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "036aed92-8f63-4350-b37d-60b119c1e00b",
   "metadata": {},
   "source": [
    "1) Fill in the following sentence:  Children of short parents became ... on average and children of tall parents became ... on average."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5c7ec6f-9078-43a4-b624-4c3df80103e3",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2a15a7c3-9a36-4820-9b67-d7321a4c9ef8",
   "metadata": {},
   "source": [
    "Why did Galton call it \"Regression towards mediocrity in hereditary stature\" which was later shortened to \"regression to the mean\"?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32fed9c3-de8e-4de1-ab19-a6744e99168b",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e23787de-1010-4ebf-8b47-97456b455f92",
   "metadata": {},
   "source": [
    "Why should this effect be real?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "265e1266-3ddb-4e0b-ab85-7cd6d2258870",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1e68d926-8003-486e-ab70-890a633987cd",
   "metadata": {},
   "source": [
    "You now have unlocked the mystery. Why is it that when modeling with y continuous, everyone calls it \"regression\"? Write a better, more descriptive and appropriate name for building predictive models with y continuous."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bcf3f29-cf26-4ec4-888d-feb73ffaeded",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "537ba2a7-b481-41eb-8fd7-f966536c2c50",
   "metadata": {},
   "source": [
    "You can now clear the workspace by restarting the kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "311d3bab-de5c-462d-a56e-a7ec8210f5ec",
   "metadata": {},
   "source": [
    "Create a dataset D which we call `Xy` such that the linear model has R^2 about 50\\% and RMSE approximately 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bac8ce44-aa09-49da-8e53-8dac48fbb139",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "980e42a3-b741-49a9-aab5-2c3feba6eb40",
   "metadata": {},
   "source": [
    "Extra credit (+5): Create a dataset D and a model that can give you R^2 arbitrarily close to 1 i.e. approximately 1 - epsilon but RMSE arbitrarily high i.e. approximately M."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "082279a7-02f8-4864-91ed-5079bf4a7c45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
